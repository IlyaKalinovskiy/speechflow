### Model configuration ###

experiment_name: prosody_predictor # name for current experiment

dirs:
  logging: experiments

seed: 1234

batch:
  type: ProsodyPredictionProcessor

data_loaders:
  batch_size: { default: 32, debug: 4 }
  min_batch_size: 4
  min_prefetch_factor: { default: 50, debug: 1 }
  max_prefetch_factor: { default: 150, debug: 1 }

trainer:
  accelerator: gpu
  devices: [auto]
  max_epochs: 40
  gradient_clip_val: 5.0
  accumulate_grad_batches: 1

checkpoint:
  monitor: Epoch
  mode: max
  save_top_k: 20
  every_n_epochs: 5
  save_last: True

optimizer:
  method:
    type: Adam
    lr: 1.e-5
    weight_decay: 0.06
  lr_scheduler:
    type: WarmupInvRsqrtLR
    lr_max: 1.e-5

callbacks:
  ProsodyCallback:
    tokenizer: {
      default: /src/libs/text_parser/text_parser/data/common/xlm-roberta-large/tokenizer,
      ru: /src/libs/text_parser/text_parser/data/ru/homo_classifier/tokenizer,
      en: /src/libs/text_parser/text_parser/data/en/roberta_large/tokenizer,
      ml: /src/libs/text_parser/text_parser/data/common/xlm-roberta-large/tokenizer,
      }

loss:
  type: ProsodyPredictionLoss
  names: [binary, category]

model:
  type: ProsodyModel
  params:
    model_name: {
      default: /src/libs/text_parser/text_parser/data/common/xlm-roberta-large/model,
      ru: /src/libs/text_parser/text_parser/data/ru/homo_classifier/ruRoBerta,
      en: /src/libs/text_parser/text_parser/data/en/roberta_large/model,
      ml: /src/libs/text_parser/text_parser/data/common/xlm-roberta-large/model,
      }
    dropout: 0
    n_classes: 10
    n_layers_tune: 15
    classification_task: both
