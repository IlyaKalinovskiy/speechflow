### Model configuration ###

experiment_name: whisp_vocos_norm_rnn_nsf

dirs:
  logging: whisp_experiments

seed: 1234

batch:
  type: VocoderBatchProcessor

data_loaders:
  batch_size: { default: 8, debug: 2 }
  epoch_len: { train: 10000, test: ~ }
  min_batch_size: 2
  min_prefetch_factor: { default: 50, debug: 1 }
  max_prefetch_factor: { default: 150, debug: 1 }

trainer:
  accelerator: { default: gpu, debug: cpu }
  devices: { default: [auto], debug: 1 }
  max_steps: 8000000
  accumulate_grad_batches: 1
  limit_val_batches: 100
  log_every_n_steps: 100
  # resume_from_checkpoint: /path/to/checkpoint

checkpoint:
  monitor: val_loss
  filename: vocos_checkpoint_{epoch}_{step}_{val_loss:.4f}
  save_top_k: 3
  save_last: true

callbacks:
  LearningRateMonitor: {}
  ModelSummary: {}
  GradNormCallback: {}
  VisualizerCallback: {}

engine:
  class_name: VocosLightningEngine
  init_args:
    sample_rate: 24000
    initial_learning_rate: 5.e-4
    mel_loss_coeff: 10.0
    mrd_loss_coeff: 0.1
    auxiliary_loss_coeff: 1.0
    decay_mel_coeff: false
    num_warmup_steps: 0  # Optimizers warmup steps
    pretrain_mel_steps: 0  # 0 means GAN objective from the first iteration
    use_sm_loss: true
    use_wavlm_loss: true
    use_cdpam_loss: false
    biometric_model_name:
      immers: /media/i.kalinovskiy/GitLab2/speechflow/wespeaker-voxceleb-resnet34-LM
      justai: /src/wespeaker-voxceleb-resnet34-LM
      debug: C:/FluentaAI/wespeaker-voxceleb-resnet34-LM
    wavlm_model_name:
      immers: /media/i.kalinovskiy/GitLab2/speechflow/wavlm-base-plus
      justai: /src/wavlm-base-plus
      debug: C:/FluentaAI/wavlm-base-plus

    # automatic evaluation
    evaluate_utmos: false
    evaluate_pesq: true
    evaluate_periodicty: true

    use_clearml_logger: { default: false, immers: true }

model:
  feature_extractor:
    class_name: AudioFeatures
    init_args:
        input_feat_type: ssl_feat
        input_proj_dim: 256
        inner_dim: 512

        ssl_feat_dim: 1024

        speaker_emb_dim: 256
        speaker_emb_proj_dim: 128

        feat_encoder_type: RNNEncoder
        feat_encoder_num_layers: 2
        feat_encoder_inner_dim: 512
        feat_encoder_use_condition: true
        feat_condition_type: adanorm

        vp_encoder_type: AdainEncoder
        vp_encoder_num_layers: 1
        vp_encoder_inner_dim: 512
        vp_encoder_use_condition: true
        vp_encoder_params:
          adain_upsample: true

        # vq_encoder_type: RNNEncoder
        # vq_encoder_use_condition: true
        # vq_num_quantizers: 3
        # vq_codebook_size: 256

        # sf_encoder_type: RNNEncoder
        # sf_encoder_num_layers: 1
        # sf_encoder_inner_dim: 512
        # sf_encoder_use_condition: true

        style_encoder_type: StyleTTS2
        style_feat_type: mel_spectrogram
        style_emb_dim: 128
        style_use_gmvae: false

        energy_denormalize: true
        energy_smooth_l1_beta: 0.1
        pitch_denormalize: true
        pitch_smooth_l1_beta: 0.1

        use_lang_emb: false
        use_speaker_emb: true
        use_speech_quality_emb: true
        use_style: true
        use_energy: true
        use_pitch: true
        use_plbert: true
        use_averages: true
        use_vq: false
        use_upsample: false
        use_sf_encoder: false

        use_auxiliary_classification_loss: false
        use_auxiliary_linear_spec_loss: false
        use_auxiliary_mel_spec_loss: false

  backbone:
    class_name: DummyBackbone  # VocosBackbone
    init_args:
      input_dim: 512
      inner_dim: 512
      # num_layers: 4
      # intermediate_dim: 1536
      # condition_dim: 256

  head:
    class_name: NSFiSTFTHiFiGANHead
    init_args:
      input_dim: 512
      condition_dim: 256
      upsample_rates: [10, 6]  # hop=240
      upsample_kernel_sizes: [20, 12]
      istft_n_fft: 20
      istft_hop_size: 4
      adain_upsample: true
